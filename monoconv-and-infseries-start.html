<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algebraic and Order Limit Theorems Exercises</title>
    <script>
        MathJax = {
            loader: {
            load: ['[custom]/xypic.js'],
            paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}
          },
          tex: {
            packages: ['base', 'ams', 'xypic'],
            tags: 'none',
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          }
        };
      </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <template>
        <div class="dropdown-box">
            <h3 class="dropdown-box-title">
            </h3>
            <div class="dropdown-box-content"></div>
        </div>
    </template>
    <template>
        <div class="spoiler">
            <div class="spoiler-head"></div>
        </div>
    </template>

    <script>
        let important_statements = new Map();
        function add_important_statements()
        {
            let impstats = document.getElementsByClassName("important-statement");
            for (let i = 0; i < impstats.length; ++i)
        }
    </script>

    <style>
        body {
            font-size: 20px; /* Adjust the font size to your preference */
        }
        img {
            max-width: 100%;
        }
        .MathJax, .overflow-hidden {
            overflow-x: auto;
            overflow-y: hidden;
        }
        ul {
            padding-left: 1em;
        }
        ol {
            padding-left: 2em;
        }
    </style>
</head>
<body>
    <div class="content">
        <h1>The Monotone Convergence Theorem and a First Look at Infinite Series.</h1>
        <b>Definition 2.4.1.</b> A sequence $(a_n)$ is <i>increasing</i> if $a_n \leq a_{n+1}$ for all $n \in \mathbb{N}$ and 
        <i>decreasing</i> if $a_n \geq a_{n + 1}$ for all $n \in \mathbb{N}. A sequence is <i>monotone</i> if it is either increasing or decreasing. <br>
        <br>
        <b>Theorem 2.4.2 (Monotone Convergence Theorem).</b> <i>If a sequence is monotone and bounded, then it converges.</i> <br>
        Proof: <br>
        Hmm.. The first way I can think of proving this is using a binary search combined with the nested interval property.
        The strategy is to take a closed interval which contains the entire sequence, 
        and have an inductive process which divides the current set into two closed intervals (so it has to intersect in the middle and that's fine). Atleast one of the sets must contain an infinite number of terms.
        After that, you would prove that only one of the set contain an infinite number of terms if there aren't an infinite number of terms in the intersection of the two sets.
        This is done by taking the smallest term on the closed interval that is in the direction that the sequence moves in (the right set if increasing, left if decreasing)
        and since every term in the other set is greater/less than it, then on the other set, there must only be the terms before the chosen term (since anything after has the opposite less/greater than relation) which means there are only finitely many terms.
        If there are an infinite number of the terms in the intersection, then you can do something similar, but now with just the intersection set and the terms on the side in the opposite direction of the direction that the sequence moves in.
        Get the smallest term in the intersection, then conclude that there are only finite amount of terms not in the intersection.
        Then prove that there are no terms after it therefore that intersection is the limit. You can say that this case is a possible end of the inductive process.
        In the other case where this never happens, the nested interval property will be used to get the limit after the inductive process.
        Lemma: If a set contains an infinite number of terms, then it's compliment doesn't.
        <br>
        <br>
        And that is when my brain finally went onto the idea of using supremum and infimums. Why do I keep overcomplicating things.
        If it is increasing, then you can take the supremum of the set containing all terms in the sequence. By definition, 
        any $x$ less than the supremum satisfy there exists an element in the set, which is equivalent to a term in the sequence,
        such that it is higher than $x$. Since every term after that is even higher then by the order limit theorem the sequence converges to something greater than $x$
        and this is true for any $x$ less than the supremum. The value is also less than or equal to the supremum because every term is less than the supremum and so order limit theorem implies that.
        <ul>
            <li>Define $X = \{ x_n | n \in \mathbb{N} \}$</li>
            <li>If $(x_n)$ is increasing
                <ul>
                    <li>Set $s = \sup X$</li>
                    <li>Take any $\epsilon > 0$</li>
                    <li>$s - \epsilon < s$ therefore there exists an $N$ such that $s - \epsilon < x_N$</li>
                    <li>For all $n \geq N$, $x_n \geq x_N$ and since $x_n \in X$, $x_n \leq s$</li>
                    <li>Since $s - \epsilon < x_N \leq x_n \leq s < s + \epsilon$ then $|s - x_n| < \epsilon$</li>
                    <li>It can be concluded that the sequence converges to $s$</li>
                </ul>
            </li>
            <li>If $(x_n)$ is decreasing... I don't think it's necessary to cover this since it can just be a copy paste of the previous part just with the supremum and signs changed</li>
        </ul>
        <br><br>
        <b>Definition 2.4.3 (Convergence of a Series).</b> Let $(b_n)$ be a sequence. An <i>infinite series</i> is a formal expression of the form <br>
        $$\sum_{n=1}^{\infty} b_n = b_1 + b_2 + b_3 + b_4 + b_5 + ...$$
        We define the corresponding sequence of partial sums $(s_m)$ by
        $$s_m = b_1 + b_2 + b_3 + ... + b_m$$
        and say that the series $\sum_{n=1}^{\infty} b_n$ converges to $B$ if the sequence $(s_m)$ converges to $B$. In this case we write $\sum_{n=1}^{\infty} b_n = B$.
        <br><br>
        <b>Theorem 2.4.6 (Cauchy Condensation Test).</b> Suppose $(b_n)$ is decreasing and satisfies $b_n \geq 0$ for all $n \in \mathbb{N}$. Then, the series $\sum_{n=1}^{\infty}$ converges if and only if the series
        $$\sum_{n=0}^{\infty} 2^n b_{2^n} = b_1 + 2b_2 + 4b_4 + 8b_8 + 16b_16 + ...$$
        converges.
        <br>
        I am EXTREMELY struggling to prove this. 
        Proving it in one direction where the "cauchy sum" converges implies the original sum converges is simple.
        We're jumping terms yet on each term we are multiplying it literally just enough to replace the missing terms such that the replacement is bigger.
        Therefore the cauchy sum is greater than the original sum and so the original sum converges.
        The other way, however, is very hard for me to do. 
        <br> <br>
        If a sequence is strictly decreasing, then it's sum converges if and only if the supremum of every ratio between two terms is less than one.
        That is something that feels intuitive to me. What I'm thinking of next is to find a condition of convergence for a similar scenario but the sequence is decreasing but not strictly.
        The condition I'm thinking of should have something to do with, for each group of terms where the ratio between them is one, how many terms are in that group?
        How many terms can all groups which satisfy that condition have such that the sum of the sequence still converges?
        If I decompose any of such sequences into the biggest subsequence such that the ratio between any two terms is less than one, 
        and have another condition which is that the decomposed sequence must have a ratio supremum less than one, then 
        what is a nice intuitive condition of convergence if we individually add a scalar to each term? This is a clearly equivalent question.
        My intuition simply based on how exponentials are related to sums so far tells me that it has to be exponential.
        Clearly a constant scalar is not good enough because it would be equivalent to multiplying the final value.
        What about a polynomial? One identifying property of them is that if you keep taking the "nth degree" differences, an analog to nth derivative for discrete sequences, then it will eventually be zero after doing it a finite amount of times and if you construct a sequence for each nth degree then it will be a polynomial of degree original degree minus n.
        Additively increasing scalars is just a special case of polynomially increasing scalars so we should focus on that first. 
        Choosing the scalars to be additively increasing by one, it is clear that it's equivalent to having the original sum, added byitself but without the first term, again and again where on the nth step it is added by itself but without the first to the nth term.
        Does this sum converge or diverge? Thinking of the summed sequences as a matrix where on each nth row it contains the sequence without the first to the nth terms, you can add up along the diagonals and see that the nth diagonal is the original sum times r to the n which is just another geometric series with the starting term being the original sum.
        So yes, it converges in that case. This strengthens the idea in my mind that a polynoimally increasing one also converges.
        I want to just assume that it's true in general for polynomial and so that, in my mind, leaves us with just exponential.
        Exponential also doesn't imply divergence because the original sequence may be exponentially decreasing with a ratio of, say, one onehundreth and the exponential scalar has a ratio of only two.
        But it feels like now we know that it's probably, most likely, at least exponential it has to be.
        What was the point of all of this? I don't know but I did end up thinking of many interesting things because of it.
        Now I'm interested in having the intuition for a proof that a polynomially increasing scalars for a convergent sequence as I've described before always converges as well.
        Actually yeah you can easily use the ratio test for that, but I did think about an unnecessarily really complicated proof of it which involves combinatorics and "nested sums" (like a second degree nested sum with m terms is (1) + (1 + 2) + (1 + 2 + 3) + ... + (1 + 2 + 3 + ... + m)).
        Basically, such a nested sum with degree d as a function of the amount of terms is the same as a polynomial of degree d + 1.
        The plan is to use combinatorics' k choose n combination stuff to prove that.
        All of that means you can make any nth degree polynomial with, like, about n nested sums. 
        Then, by taking advantage of nested sums being nested, you can prove that for each nested sum of degree d,
        a convergent geometric series term-wise multiplied by such a nested sum converges.
        This is done by expanding the nested sum in each term in the extended geometric series,
        and notice that if you visualize it as an infinite top-right triangle matrix summation,
        you can sum along the diagonals and notice each nth diagonal is the sum of the first diagonal multiplied by the geometric sum's ratio to the n minus one.
        But the first diagonal itself is a convergent geometric sum multiplied by a nested sum but now with degree d - 1.
        It is known that if that converges, then the outer sum converges. But you can keep expanding these nested-sum-geometric-sum series until it's just a geometric series
        which means the bottom converges, but the one right above it also converges, then the one above as well since each is just a geometric series and so on. 
        So the final thing converges.
        <br><br>
        Anyway that's enough brainstorming. All of it ended up being irrelavant since I just managed to think about a really simple way of proving this.
        <!-- If the "cauchy sum" diverges, then it's UNBOUNDED and the corresponding sequence which represents the series is always increasing.
        The strategy is to substract the cauchy sum by $b_1$, then divide by two, then add by $b_2$ which makes the sum be equal to:
        $$
        2b_2 \sum_{i=1}^{\infty} 2^{i}b_{2^{i + 1}} = 2b_2 + 2b_4 + 4b_8 + ... 
        $$
        Up until the first term, it misses $b_1$ but it tries to make up for it with $b_2$ which is less than $b_1$.
        Then between the first and second term, it misses $b_3$ yet makes up for it with $b_4$ which again is less than $b_3$.
        Then between the third and second term, it misses $b_5, b_6, b_7$ yet it makes up for it with $b_8$ which is less than each of those.
        This is just saying that, if $(c_n)$ is the corresponding sequence for the cauchy sum, then $c_n < s_{2^{n + 1}}$.

        Since it is unbounded, you can take any $\epsilon > 0$ and know that there exists an $N \in \mathbb{N}$ such that for all $n \geq N$, $|c_n| \geq |\epsilon|$.
        Which also means $|s_{2^{n + 1}}| \geq \epsilon$ and so the original sum is also unbounded.
        Anyway I'll do all of the proof formally in the next section.
        <br><br>
        Define $c_n = \sum_{i=1}^{n} 2^{i - 1} b_{2^{i - 1}}$ therefore the Cauchy sum from earlier is equal to the limit of that.
        This also means $c_n = c_{n-1} + 2^{n - 1} b_{2^{n - 1}}$ and $c_1 = b_1$.
        Take any natural $N$ and then the $2^{N-1}$th term of $b$. It is known that, for all natural $n$, $b_{2^{N-1} + n - 1} \leq b_{2^{N - 1}}$.
        Therefore, $\sum_{i=1}^{2^{N-1}} b_{2^{N-1} + i - 1} \leq 2^{N-1} b_{2^{N - 1}}$.
        It is known that $c_1 = b_1 \geq s_1 = \sum_{i=1}^{1} b_i$.
        Suppose, for any natural $n$, $c_n \geq s_{2^n - 1}$, then
        $c_{n+1} = c_n + 2^{n} b_{2^{n}} \geq s_{2^n - 1} + \sum_{i=1}^{2^{n}} b_{2^{n} + i - 1}$.
        The expression on the right is equivalent to $\sum_{i=1}^{2^{n} - 1} + \sum_{i=2^{n}}^{2^{n+1} - 1} b_{i}$
        which may be combined to get $\sum_{i=1}^{2^{n + 1} - 1} b_i = s_{2^{n + 1} - 1}$.
        Therefore, $c_{n+1} \geq s_{2^{n + 1} - 1}$ and by induction you can conclude that, for any $n$,
        $c_{n+1} \geq s_{2^{n + 1} - 1}$.
        <br> -->
        <ul>
            <li>Define $c_n = \sum_{i=1}^{n} 2^{i - 1} b_{2^{i - 1}}$ <br>
                Therefore $\lim c_n = \sum_{i=1}^{\infty} 2^{i - 1} b_{2^{i - 1}}$ which is equivalent to the Cauchy sum <br>
                Also that $c_1 = b_1$ and $c_n = c_{n-1} + 2^{n - 1} b_{2^{n - 1}}$ is an equivalent definition of $(c_n)$.
            </li>
            <li>It is known that for all natural $N$ and $n$, $b_{2^{N-1} + n - 1} \leq b_{2^{N - 1}}$</li>
            <li>This implies $\sum_{i=1}^{2^{N-1}} b_{2^{N-1} + i - 1} \leq 2^{N-1} b_{2^{N - 1}}$
                because the sum contains $2^{N-1}$ terms and every term is less than or equal to $b_{2^{N-1}}$
            </li>
            <li>Induction for $c_{n} \geq s_{2^{n} - 1}$ for all natural $n$
                <ul>
                    <li>For $n = 1$: $c_1 = b_1 \geq s_1 = \sum_{i=1}^{1} b_i$</li>
                    <li>For any $n \in \mathbb{N}$: Suppose, for any natural $n$, $c_n \geq s_{2^n - 1}$, then
                        $$
                        \begin {align*}
                        c_{n+1} = c_n + 2^{n} b_{2^{n}} & \geq s_{2^n - 1} + \sum_{i=1}^{2^{n}} b_{2^{n} + i - 1} \\
                        & = \sum_{i=1}^{2^{n} - 1} b_i + \sum_{i=2^{n}}^{2^{n+1} - 1} b_{i} & \text{ expansion of $s_{2^{n} - 1}$ and change of variables for the right sum} \\
                        & = \sum_{i=1}^{2^{n+1} - 1} b_i & \text{ the sums are combined} \\
                        &= s_{2^{n + 1} - 1}
                        \end {align*}
                        $$
                        therefore $c_{n+1} \geq s_{2^{n + 1} - 1}$
                    </li>
                    <li>By induction, it can be concluded that, for all natural $n$, $c_{n} \geq s_{2^n - 1}$</li>
                </ul>
            </li>
            <li>Since $(c_n)$ converges, it is bounded and so there exists an $M > 0$ such that, for every natural $n$, $|c_n| < M$</li>
            <li>It is clear that $(s_n)$ is increasing. Take any natural $n$, $2^{n} - 1 \geq n$ and so $c_n \geq s_{2^{n} - 1} \geq s_n$</li>
            <li>Therefore $M \geq |c_n| \geq |s_n|$ meaning $s_n$ is also bounded.</li>
            <li>By the MCT, since $s_n$ is bounded and monotone, then it is convergent.</li>
        </ul>
        <!-- The strategy for the next part is that it is known that if the Cauchy sum diverges, then it is unbounded by the MCT.
        Then create a slightly modified version of the Cauchy sum such that for any nth term in the modified sum, it is easy to find a corresponding term in the sequence which represents the original series
        that is less than that nth term. The change in the modified version is also small enough for it to be easy to prove that it is also unbounded.
        This would imply that the original series is also unbounded. -->
        The strategy for the next part is that it is known that if the Cauchy sum diverges, then it is unbounded by the MCT.
        Then you can show that, if you take the original sum multiplied by two, then for each partial sum of the cauchy sum, you can
        easily find a partial sum for the original sum times two such that the cauchy partial sum is less than it.
        This means that the original sum is also unbounded.
        <ul>
            <li>Suppose $(c_n)$ diverges therefore the negation of the MCT implies that $(c_n)$ is either unbounded or not monotone.
                Since it is monotone (increasing) then it is known that <i>$(c_n)$ must be unbounded.</i>
            </li>
            <li>
                For any $n$, $c_n \leq 2s_{2^{n - 1}}$:
                <ul>
                    <li>For $n = 1$: $c_1 = b_1 \leq 2s_1 = 2b_1$</li>
                    <li>For any $n \in \mathbb{N}$: If $c_n \leq 2s_{2^{n-1}}$:
                        <ul>
                            <li>It is known that, for all $n \geq 2^{N-1} + 1$, $b_n \leq b_{2^{N-1} + 1}$</li>
                            <li>Since $c_{n+1} = c_n + 2^{n} b_{2^n}$ and $s_{2^{n}} = s_{2^{n-1}} + \sum_{i=2^{n-1} + 1}^{2^n} b_i$,
                                it is enough to show that $2^{n} b_{2^n} \leq 2 \sum_{i=2^{n-1} + 1}^{2^n} b_i$
                            </li>
                            <li>The sum for $s_{2^{n}}$ has $2^{n-1}$ terms, and every term inside of the sum is geq than $b_{2^{n}}$</li>
                            <li>Therefore $\sum_{i=2^{n-1} + 1}^{2^n} b_i \geq 2^{n-1} b_{2^{n}}$</li>
                            <li>It can be concluded that $2 \sum_{i=2^{n-1} + 1}^{2^n} b_i \geq 2^n b_{2^{n}}$</li>
                            <li>Therefore $c_{n+1} \leq 2s_{2^n}$</li>
                        </ul>
                    </li>
                    <li>By induction, $c_n \leq 2s_{2^{n-1}}$ for all natural $n$</li>
                </ul>
            </li>
            <li>Pick any $M > 0$, since $c_n$ is unbounded there exists an $n$ such that $|c_n| \geq 2M$ therefore $|s_{2^{n-1}}| \geq M$ and so $(s_n)$ is also unbounded.</li>
        </ul>
        <br><br>
        <b>Corollary 2.4.7.</b> The series $\sum_{n=1}^{\infty} \frac{1}{n^p}$ converges if and only if $p > 1$. <br>
        My initial intuition about this was that this is false. I thought about something similar which is that
        such a series always diverges before reading this corollary. My logic was that if a sequence is decreasing
        and is always in $[0, 1)$, then it's sum will always diverge if the ratio between two terms is always increasing and the supremum of every ratio between two terms is one.
        This should be the case for such a series but apparently it may converge.
        The reason I thought that was intuitive was beacuse, if that's the case, then for any $x \in (0, 1)$ there's a starting term $N$ such that the ratio between two terms for every term after it is greater than $x$.
        So you can take, say, any $k \in \mathbb{N}$ and then a ratio of $\frac{10^{k} - 1}{10^k}$ and so by the geometric series sum formula,
        it would converge to $a \cdot 10^k$ where $a$ is about the $N$th term.
        However I guess I did forget to think about the fact that the $N$th term also always gets smaller...
        The book says that this corollary will be proved later on an exercise in section 2.7.
        <br><br>
        <b>Exercise 2.4.1.</b> (a) Prove that the sequence defined by $x_1 = 3$ and
        $$x_{n+1} = \frac{1}{4 - x_n}$$ converges.
        
        <br><br>
        <!-- This seems like a good opportunity to generally talk about recursive sequences defined as
        $x_{n+1} = \frac{1}{r - x_n}$.
        Suppose $0 < x_n < r - \frac{1}{r}$, then $r > r - x_n > \frac{1}{r}$, then
        $\frac{1}{r} < \frac{1}{r - x_n} < r$ which then doesn't satisfy the previous condition anymore
        unless the original upper bound gets changed to $r - \frac{1}{r - \frac{1}{r}}$. But then even with that, after two iterations,
        it doesn't satisfy the original equation either. What if you define $x = r - \frac{1}{x}$, would that work as an upper bound?
        It feels like it should. Let's say we can be certain that $x = r - \frac{1}{x}$ exists for all $|r| > 2$ based on
        the fact that solving it using the quadratic formula yields an expression that is real if and only if $|r| > 2$. 
        Then does that bound really work...?
        $0 < x_n < r - \frac{1}{x}$ therefore $r > r - x_n > \frac{1}{x}$ therefore $\frac{1}{r} < \frac{1}{r - x_n} < r - \frac{1}{x}$
        which still satisfy the original inequality.
        Maybe it's even better if you set the bounds to be the two possible $x$ values.
        A natural question which comes from there is what happens when $x$ is outside of those bounds.
        Suppose $x_n > r$, then the next term is negative, making the term after that be positive but less than $\frac{1}{r}$.
        Therefore, anything negative and anything greater than $r$ ends up being in $(0, \frac{1}{r})$
        which satisfy the nice-behaved condition from earlier.
        All that is left is what is between the greatest $x$ and $r$. -->
        This seems like a good opportunity to generally talk about recursive sequences defined as
        $x_{n+1} = \frac{1}{r - x_n}$ in general. Before that, I will talk about the
        behvaiour of taking reciprocals with strict ineqalities.
        Suppose $a < x < b$, then: <br>
        1. $\frac{1}{a} < x < \frac{1}{b}$ if and only if $a < 0 < b$ <br>
        2. $\frac{1}{a} > x > \frac{1}{b}$ if and only if $a < b < 0$ or $0 < a < b$. In other words, they are of the same sign. <br>

        Now regarding the behaviour of recursively defined sequences in the form of:
        $x_{n+1} = \frac{1}{r - x_n}$
        for some real number $r$ assigned to each sequence.

        On what subset of R for $x_n$ does it converge on depending on $r$?
        What are it's limit points?

        We will limit our view to $r > 2$ for reasons that will be clear later.
        What are it's limit points? What value of $x_n$ such that $x_{n+1} = x_n$?
        You can try to solve for $x = \frac{1}{r - x}$.
        Of course, an obvious concern with finding the limit like this is that this means we are assuming that such a value exist.
        We will not concern ourselves with this for now, we will only intuitively think about why this is valid by simply verifying that the resulting value for $x$ is algebraically valid and satisfy the original equation.

        By using the quadratic equation, you can conclude that $x = \frac{r \pm \sqrt{r^2 - 4}}{2}$.
        We will attempt to verify that this is value satisfy the original equation.
        The most straightforward attempt to do this is by literally doing $\frac{1}{1 - x}$ and seeing if it is equal to $x$.
        $$
        \begin{align*}
        x &= \frac{r \pm \sqrt{r^2 - 4}}{2} \\
        r - x &= \frac{r \mp \sqrt{r^2 - 4}}{2} \\
        \frac{1}{r - x} &= \frac{2}{r \mp \sqrt{r^2 - 4}} \\
        &= \frac{2}{r \mp \sqrt{r^2 - 4}} \cdot \frac{r \pm \sqrt{r^2 - 4}}{r \pm \sqrt{r^2 - 4}} \\
        &= \frac{2 r \pm \sqrt{r^2 - 4}}{(r^2 - (r^2 - 4))} \\
        &= \frac{2 r \pm \sqrt{r^2 - 4}}{4} \\
        &= \frac{r \pm \sqrt{r^2 - 4}}{2} \\
        &= x
        \end{align*}
        $$
        and so, yes, such an $x$ really does satisfy the original equation. We now intuitively know the limit points of the sequence.
        Now it's clear that the reason we used $r \geq 2$ earlier is that, if it less than two, then based on this formula,
        the limit points will not be defined. Actually, $|r| \geq 2$ would also work fine, so we will partially do that from now on.
        Now we will actually start investigating the convergence of this sequence based on the starting point $x_n$.
        
        <ul>
            <li>Define $k_d = sqrt{r^2 - 4}$ therefore the limit point of $(x_n)$ is now $\frac{r \pm k_d}{2}$</li>
            <li>It's also clear that $k_d < |r|$</li>
            <li>Define $k_1, k_2$ to be limit points of $(x_n)$ such that $|k_1| < |k_2|$ if $|r| > 2$ and $k_1 = k_2$ if $|r| = 2$.
                This implies $k_1, k_2$ is both in the form of $\frac{r \pm k_d}{2}$ but with the $\pm$ changed into either $+$ or $-$ depending on the sign of $|r|$.
            </li>
            <li>Properties of $k_1, k_2$
                <ul>
                    <li>$|k_1|, |k_2| < |r|$ and $|k_1 - r|, |k_2 - r| < |r|$ in other words $k_2$ and $k_1$ is between 0 and $r$<br>
                        It is clear that $|k_d| = |\sqrt{r^2 - 4}| < r$. By the triangle inequality, $|\frac{r \pm k_d}{2}| < |\frac{r}{2}| + |\frac{k_d}{2}| < |r|$.
                        We also already know that $$
                        End of proof.
                    </li>
                    <li>$r - k_1 = \frac{1}{k_1}$ and $r - k_2 = \frac{1}{k_2}$ <br>
                        This is because if $k$ is a number which satisfies $\frac{1}{r - k} = k$, which we know is true for $k_1, k_2$, then it implies $r - k = \frac{1}{k}$. 
                    </li>
                    <li>$|\frac{1}{r}| < |k_2|$ <br>
                        This is beacuse $|r| \geq 2$ meaning $\frac{1}{r} < 1$
                        yet since $|k_2| = |r| + |k_d|$
                    </li>
                </ul>
            </li>
            <li>Useful preservation of inequality property: When $r \geq 2$ and $0 \leq a < b < r$, then $0 \leq \frac{1}{r - a} < \frac{1}{r - b}$
                and when $r \leq 2$ and $r < a < b \leq 0$, then $\frac{1}{r - a} < \frac{1}{r - b} < 0$
                In other words, the sequence operation preserves inequalities on values between zero and $r$.
                This is because in the case where $r \geq 2$, $r \geq r - a > r - b > 0$ which means everything still has the same sign and so taking it's reciprocal reverts it to the original inequalities.
                You can also do something similar for when $r \leq 2$.
            </li>
            <li>When $x_n$ is inbetween $k_2$ and $0$, then $x_{n+1}$ is also inbetween $k_2$ and $0$.
                This is because suppose $0 < x_n < k_2$, then $r - \frac{1}{r} < x_{n+1} < k_2$ and it's also similar for when $k_2 < x_n < 0$.
                This implies the sequence is monotone when $x_n$ is betewen $k_2$ and $0$. 
                This is because suppose $x_{n} < x_{n+1}$ then since both of them is between $0$ and $r$ (because $k_2$ is between $0$ and $r$) then the inequality is preserved under the sequence operation
                therefore $x_{n+1} < x_{n+2}$. It's also similar for when $x_{n} > x_{n+1}$.
                This implies that $x_n$ is monotone and bounded starting from the $n$ where $x_n$ is inbetween zero and $k_2$ and so by the MCT, it is convergent.
            </li>
            <li>When $x_n$ is of different signs than $r$, then $x_{n+1}$ will be of the same sign as $r$ and is inbetween $r$ and zero.
                This is because suppose $r > 0$, then when $x_n < 0$, $r - x_n > r$ and so $x_{n+1} < \frac{1}{r}$.
                Now since $\frac{1}{r}$ is actually inbetween $r$ and $k_2$ which would imply that $x_{n+1}$ is inbetween zero and $k_2$
                meaning you can fallback to the previous case and so $x_n$ converges.
            </li>
            <li>When $x_n < 0$, $r - x_n > r$ therefore $0 < \frac{1}{r - x_n} < \frac{1}{r}$ and so you can fallback into the previous case since $\frac{1}{r} < k_2$</li>
            <li>When $x_n > r$, $r - x_n < 0$ and so $x_{n+1} < 0$ therefore you can fallback into the previous case</li>
            <li>When $k_2 < x_n < r$, it's complicated, but here's the proof anyway
                <ul>
                    <li>Define $g(x) = r - \frac{1}{x}$ therefore it is the inverse of the sequence operation $\frac{1}{r - x}$.</li>
                    <li>Useful property: $g(k_2) = k_2$ and $g$ also preserves inequalities for values between zero and $r$ since it is the inverse of another operation which does that</li>
                    <li>Define the sequence $(g_n)$ such that $g_1 = r$ and $g_{n+1} = g(g_n)$</li>
                    <li>$(g_n)$ is dereasing <br>
                        Clearly $g_2 = r - \frac{1}{r} < r = g_1$ therefore $g_2 < g_1$.
                        UNFINISHED
                    </li>
                    <li>$(g_n)$ is bounded below by $k_2$:
                        Clearly, $g_1 = r > k_2$ therefore $g(g_1) = g_2 > k_2$
                        and in general, $g_n > k_2$ imply $g_{n+1} > k_2$ since $g(k_2) = k_2$
                        NEEDS IMPROVEMENT
                    </li>
                    <li>Since $(g_n)$ is decreasing and bounded below by $k_2$, then by the MCT, it converges.</li>
                    <li>Define $p = \lim g_n$</li>
                    <li>Proof of convergence when $p < x_n < r$
                        <ul>
                            <li></li>
                        </ul>
                    </li>
                    <li>Proof of convergence when $k_2 < p$ <br>
                        Before I go onto the proof, it is presumed that $k_2 = p$ must be true,
                        but I don't know how to prove that so...
                        <ul>
                            <li></li>
                        </ul>
                    </li>
                </ul>
            </li>
        </ul>

        all of the proof above is valid for $r > 2$, but is very easily changed to also be valid for $r < 2$ by simply 
        switching all of the inequalities.
        I'm still really bad with triangle inequalities so yeah I didn't even try to make the proof more general by using that.
        That would be absolute hell for me.
        
        <!-- Before that, let's define $k_1, k_2$ to be the limit points of the sequence.
        If $|r| > 2$, set $|k_1| < |k_2|$, we know that this is possible. If $|r| = 2$, then $k_1 = k_2$.
        Refer back to the behaviour of taking reciprocals. 
        They imply that you can't know the actual behaviour of a variable under taking reciprocals based on it's bounds.
        You have to make sure that the value also satisfy some other constraints.
        Say we try to check what happens if $a < x_n < b$, then you will also have to find out what exactly
        $r - x_n$ is and you can't immediately just take it's reciprocal and then be sure that it still satisfy the final bounds.
        To help with dealing with that later on, we will find out about if $k_1, k_2 < r$.
        Well, it's clear that $\sqrt{r^2 - 4} < |r|$, therefore $\frac{r + \sqrt{r^2 - 4}}{2} < |r|$.
        This also implies that $\frac{r - \sqrt{r^2 - 4}}{2} < |r|$.
        If $r$ is positive, then both of those expressions are also positive.
        And if $r$ is negative, it's also clear that both of them will stay negative.
        Meaning, for any $r_1 < x_n < r_2$, taking the reciprocal of $r - x_n$ always behaves in a nice way.
        <ul>
            <!-- <li>Suppose $0 < |x_n| < |r|$ and $0 < |r - x_n| < |r|$ which just means the signs of $x_n$ and $r$ is the same.
                It is equivalent to saying that $x_n$ is always between $r$ and zero. <!-- if $r > 0$, then $r > x_n > 0$ and if $r < 0$, then $r < x_n < 0$. -- >
                This implies that $\infty > \frac{1}{|r - x_n|} > \frac{1}{|r|}$ and that their signs stay the same.
                Which means $|x_{n+1}| > |\frac{1}{r}|$.
                Since $r \geq 2$, then it's reciprocal is less than itself meaning it is possible for the next term to still be between $r$ and zero
                which means that case is now already handled. What we need now is to know what happens when $|x_n| > |r|$ and their signs are the same.
            </li>
            <li>
                Suppose $|x_n| > |r|$ and $|r - x_n| < |x_n - 0|$. The latter is the same as saying that $x_n$ is always closer to $r$ than to zero which, combined with the 
                other condition, is equivalent to saying that if $r$ is negative, then $x_n$ is less than $r$ and if $r$ is positive, then $x_n$ is greater than $r$.
                Intuitively, we know that $r - x_n$ would result in a value that is of the opposite sign of $r$ which would mean that $|r - x_{n+1}|$ would result in a value greater than $|r|$ making the next value, $x_{n+2}$
                have the same sign as $r$ yet still less than $r$ meaning we can fallback into the previous case.
            </li> -- >
            <li>Suppose $x_n$ is between $0$ and $k_2$. More formally, $|x_n| < |k_2|$ and $|x_n - k_2| < |k_2|$.
                What we will need to show next is the value of $r - x_n$ and whether or not it will behave nicely with taking reciprocals
                in the sense that there is only one possibility regardless of what $x_n$ is if it satisfy the specified conditions.
                It is known that $|k_2| < |r|$ and they are of the same sign which implies that |r - k_2| < |r|.
            </li>
        </ul> -->
        <ul></ul>
        <br>
        <br>
        I give up already lol I'm just going to prove that this specific sequence is monotone and bounded now.

        <br><br>
        <b>Exercise 2.4.6 (Arithmetic-Geometric Mean).</b>
        (a) Explain why $\sqrt{xy} \leq (x + y)/2$ for any two positive real numbers $x$ and $y$.
        (The geometric mean is always less than the arithmetic mean.)
        (b) Now let $0 \leq x_1 \leq y_1$ and define
        $$x_{n+1} = \sqrt{x_n y_n} \quad \text{and} \quad y_{n+1} = \frac{x_n + y_n}{2}$$
        Show $\lim x_n$ and $\lim y_n$ both exist and are equal.
        <br><br>
        (a) Start from $x_1 = y_1$ which means $(x_1 + y_1)/2 = \sqrt{x_1 y_1} = x_1 = y_1$.
        Now suppose $2x_1 \geq x_2 > x_1$ and $x_2 + y_2 = x_1 + y_1$ which implies that $x_2, y_2$'s' arithmetic mean is the same as $x_1, y_1$'s arithmetic mean.
        This implies that $y_2 = x_1 + y_1 - x_2 = 2x_1 - x_2 \leq 2x_1$ which means $2x_1 \geq y_2 > 0$.
        The fact that $y_2$ is positive comes from the fact that $2x_1 \geq x_2 > x_1 > 0$.
        Since $x_2 > x_1$, then $x_1 = y_1 > y_2$. 
        <br>
        Next, it will be proven that $x_2y_2 < x_1y_1$.
        $x_2y_2 = x_1y_1 + (x_2 - x_1)y_2 - (y_1 - y_2)x_1$.
        This comes from the following awfully mouse-drawn image: <br>
        <img src="images/monoconv-and-infseries-start.html"> <br>
        Now we only need to prove that $(x_2 - x_1)y_2 < (y_1 - y_2)x_1$.
        It is known from earlier that $y_2 = 2y_1 - x_2$ therefore $y_1 - y_2 = x_2 - y_1 = x_2 - x_1$.
        Since $y_2 < x_1$ is also known, then $(x_2 - x_1)y_2 < (x_2 - x_1)x_1 = (y_1 - y_2)x_1$.
        <br>
        Therefore, you can conclude that $x_2y_2 < x_1y_1$.
        You can also apply the same logic for if you pick any $x_2, y_2$ such that $y_2 > y_1$ and $x_2 + y_2 = x_1 + y_1$
        because of the symmetry of the expression.
        <br>
        This implies that the geometric mean is always less than the arithmetic mean because,
        for any choice of $x, y$, you can pick $p = \frac{x + y}{2}$ and know that
        $x, y$ satisfy either $x < p$ or $y < p$ with the arithmetic mean of $x, y$ and $p, p$ being the same,
        therefore $\sqrt{xy} < \sqrt{p^2} = \frac{x + y}{2}$. You can do that if $x \neq y$ but if that's not the case then the condition is already satisfied anyway.
        <br><br>
        (b) By the AM-GM inequality and induction, it's clear that $0 \leq x_n \leq y_n$ is true for all natural $n$.
        It is also true that both of them is monotone. Since $x_n \leq y_n$, then $y_{n+1} = \frac{x_n + y_n}{2} \leq y_n$.
        You can also use similar logic for $x_{n+1}$. Since $x_n \leq y_n$, then $x_n^2 \leq x_ny_n$ which implies $x_n \leq x_{n+1}$.
        $x_n$ is bounded above by $y_n$ yet $y_n$ is bounded above by $y_1$ and both of the sequences are bounded below by zero therefore both $(y_n)$ and $(x_n)$ is bounded.
        Since they are monotone and bounded, then they are convergent based on the MCT.
        Now to prove that they converge to the same limit. First define $l$ to be the limit of $(y_n)$.
        Take any $\epsilon > 0$ and take an $N \in \mathbb{N}$ such that for all $n \geq N$, $|y_{n} - l| < \epsilon/3$.
        This means $|\frac{x_n + y_n}{2} - l| < \epsilon/3$ and so $|x_n + y_n - 2l| < \epsilon 2/3$ therefore $\epsilon 2/3 > |x_n + y_n - 2l| > |x_n - l| - |y_n - l| > |x_n - l| - \epsilon/3$
        by the triangle inequality and by the fact that $|y_n - l| < \epsilon/3$.
        This means $|x_n - l| - \epsilon/3 < \epsilon 2/3$ and so $|x_n - l| < \epsilon$.
        Alternatively, you can also do it this way: <br>
        <img src="images/monoconv-and-infseries-start-2.html"> <br>
        The picture above essentially tries to prove that $\lim (y_n - x_n) = 0$ which, by the algebraic order limit theorems, imply that $\lim y_n = \lim x_n$.
        First, since $(y_n)$ converges, you can take an $\epsilon > 0$ and take an $N \in \mathbb{N}$ such that for all $n \geq N$, $|y_n - l| < \frac{\epsilon}{4}$.
        This means that $|y_{n+1} - l| < \frac{\epsilon}{4}$ is also true and is also shown in the image.
        It is also known that $|y_{n+1} - y_n| = |\frac{x_n}{2} + \frac{y_n}{2} - y_n| = \frac{|x_n - y_n|}{2}$.
        The image uses intuition to show that that expression is less than or equal to half of $\epsilon$ which means the difference between $x_n$ and $y_n$ is less than $\epsilon$.
        You can also describe it more formally by using the triangle inequality. Since $|y_{n+1} - y_n| = |(y_{n+1} - l) - (y_n - l)| < |y_{n+1} - l| + |y_n - l| < \frac{\epsilon}{2}$.
        Then it is also known that $|y_{n+1} - y_n| = \frac{|x_n - y_n|}{2}$ which implies that $|x_n - y_n| < \epsilon$.
        <br>
        <br>
        <b>Exercise 2.4.10 (Infinite Products).</b> A close relative of infinite series is the <i>infinite product</i>
        $$\prod_{n=1}^{\infty} b_n = b_1b_2b_3 \ddots$$
        which is understood in terms of its sequence of partial products
        $$p_m = \prod_{n=1}^{m} b_n = b_1b_2b_3 \ddots b_m$$
        Consider the special class of infinite products of the form
        $$\prod_{n=1}^{\infty} (1 + a_n) = (1 + a_1)(1 + a_2)(1 + a_3)..., \quad \text{where $a_n \geq 0$}$$
        (a) Find an explicit formula for the sequence of partial products in the case where $a_n = 1/n$ and decide whether the sequence converges.
        Write out the first few terms in the sequence of partial products in the ccase where $a_n = 1/n^2$ and make a conjecture about the convergence of this sequence. <br>
        (b) Show, in general, that the sequence of partial products converges if and only if $\sum_{n=1}^{\infty} a_n$ converges. (The inequality $1 + x \leq 3^x$ for positive $x$ will be useful in one direction.) <br>
        Side story, in the middle of doing this problem, it took me many days to realize the existance of the constraint $a_n \geq 0$. I thought that $a_n$ could be anything and I was so confused about how to prove that. <br>
        <br>
        (a) If $a_n = 1/n$, then $1 + a_n = (n+1)/n$ meaning that $\prod_{n=1}^{m} a_n = \prod_{n=1}^{m} \frac{n+1}{n} = \frac{2}{1} \cdot \frac{3}{2} \cdot ... \cdot \frac{m+1}{m} = \frac{m+1!}{m!} = m+1$.
        Meaning $p_m = m+1$ and so the sequence of partial product converges.
        The first few terms of the partial product when $a_n = \frac{1}{n^2}$ are: $a_1 = 2$, $a_2 = 2 \cdot \frac{5}{4} = \frac{5}{2}$, $a_5 = \frac{5}{2} \cdot \frac{10}{9} \cdot \frac{17}{16} \cdot \frac{26}{25} ~ 3.069$, $a_{20}  ~ 3.501$, $a_{100} ~ 3.64$, $a_{1000} ~ 3.672$, $a_{1000000} ~ 3.676$.
        The series seems to increase very slowly and so it probably converges (well we already know for sure that it does anyway.) <br>
        (b)
        Before that, notice that the sequence of partial products for $1 + a_n$ and the sequence of partial sums for $a_n$
        is always positive and increasing. Therefore we only need to prove that both of them are bounded above.
        The fact that they are both increasing also implies that $\sum_{i=1}^{m} a_i \leq \sum_{i=1}^{\infty}$ for all natural $m$ which will be used later.
        <ul>
            <li>(<=) If the infinite sum converges, then the infinite product converges
                <ul>
                    <li>Since $a_n$ is positive for all $n$, then $1 + a_n \leq 3^{a_n}$ for all $n$ by the hint given by the book</li>
                    <li>Therefore, $\prod_{i=1}^{m} (1 + a_i) \leq 3^{\sum_{i=1}^{m} a_i} \leq 3^{\sum_{i=1}^{\infty} a_i}$ for all $m$</li>
                    <li>Since the infinite sum converges, we can denote the limit of the sum to be $s$ and know that it's a real number, then it is known that $\prod_{i=1}^{m} 1 + a_i \leq 3^s$ for all $m$</li>
                    <li>Meaning the sequence of partial product is bounded above and so it converges by the MCT</li>
                </ul>
            </li>
            <li>(=>) If the infinite product converges, then the infinite sum converges <br>
                The strategy here is to prove that for any $n$, there exists a natural $m$ such that $s_n \leq p_m$ which means $s_n \leq p$ for all $n$ since, for any $m$, $p_m \leq p$.
                <ul>
                    <li>It is known that if $p_0 = 1$, then $p_n = p_{n-1} (1 + a_n)$ = p_{n-1} + p_{n-1}a_n. <br>
                        Then it is also known that if $s_1 = a_1$, then $s_n = s_{n-1} + a_n$.
                    </li>
                    <li>It is also known that $p_n \geq 1$ for all $n$ therefore $a_ip_n \geq a_i $ for all natural $i, n$</li>
                    <li>For $n = 1$, it is known that $s_1 = a_1 \leq p_1 = 1 + a_1$ therefore $s_1 \leq p_1$</li>
                    <li>Now suppose it is true for some $n$ that $s_n \leq p_n$, then $s_n + a_n \leq p_n + p_n a_{n+1}$ by bullet point two
                        therefore $s_{n+1} \leq p_{n+1}$.
                    </li>
                    <li>By induction, you can conclude that $s_n \leq p_n$ for all natural $n$</li>
                    <li>Since $(p_n)$ is increasing, then $s_n \leq p_m$ for all $m \geq n$ meaning you can use the order limit theorems to conclude that $s_n \leq p$ for all natural $n$</li>
                    <li>Since $s_n$ is bounded above $p$ and it is always increasing, you can conclude that it converges by the MCT.</li>
                </ul>
            </li>
        </ul>
    </div>
</body>
</html>